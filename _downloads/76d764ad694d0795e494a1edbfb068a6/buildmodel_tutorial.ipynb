{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheresiaQuintes/ml/blob/main/_downloads/76d764ad694d0795e494a1edbfb068a6/buildmodel_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gmTw81xOTank"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v5tiAhkTanl"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\|\n",
        "[Quickstart](quickstart_tutorial.html) \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| **Build Model** \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Build the Neural Network\n",
        "========================\n",
        "\n",
        "Neural networks comprise of layers/modules that perform operations on\n",
        "data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace\n",
        "provides all the building blocks you need to build your own neural\n",
        "network. Every module in PyTorch subclasses the\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "A neural network is a module itself that consists of other modules\n",
        "(layers). This nested structure allows for building and managing complex\n",
        "architectures easily.\n",
        "\n",
        "In the following sections, we\\'ll build a neural network to classify\n",
        "images in the FashionMNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ph2eZwbGTanm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZpqrX02Tanm"
      },
      "source": [
        "Get Device for Training\n",
        "=======================\n",
        "\n",
        "We want to be able to train our model on a hardware accelerator like the\n",
        "GPU or MPS, if available. Let\\'s check to see if\n",
        "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) or\n",
        "[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are\n",
        "available, otherwise we use the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iLXu9q_ETann",
        "outputId": "f57c5ea8-2e16-4a61-c34e-0c2c671c685c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgpXYDTATann"
      },
      "source": [
        "Define the Class\n",
        "================\n",
        "\n",
        "We define our neural network by subclassing `nn.Module`, and initialize\n",
        "the neural network layers in `__init__`. Every `nn.Module` subclass\n",
        "implements the operations on input data in the `forward` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TF8I_4tiTann"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKQ79i_cTano"
      },
      "source": [
        "We create an instance of `NeuralNetwork`, and move it to the `device`,\n",
        "and print its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rDWGxjsjTano",
        "outputId": "ebe9eb8d-3106-47c3-ffd7-617c0fd157f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwXZMfjeTanp"
      },
      "source": [
        "To use the model, we pass it the input data. This executes the model\\'s\n",
        "`forward`, along with some [background\n",
        "operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
        "Do not call `model.forward()` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0\n",
        "corresponding to each output of 10 raw predicted values for each class,\n",
        "and dim=1 corresponding to the individual values of each output. We get\n",
        "the prediction probabilities by passing it through an instance of the\n",
        "`nn.Softmax` module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cH_fsqB4Tanp",
        "outputId": "fdca695c-34ef-4ea6-f920-c0c8082f8e3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([9, 8])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(2, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits)\n",
        "print(logits.shape)\n",
        "print(pred_probab)"
      ],
      "metadata": {
        "id": "vbuSUer7VPxX",
        "outputId": "596a3217-b65c-4c0f-c5b5-ed75386de7f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0685,  0.0251, -0.0947, -0.0583, -0.0427,  0.0165,  0.0058,  0.1247,\n",
            "          0.0710,  0.1496],\n",
            "        [-0.0387,  0.0030, -0.0960, -0.0687, -0.0489, -0.0008,  0.0233,  0.0641,\n",
            "          0.0867,  0.0502]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([2, 10])\n",
            "tensor([[0.0919, 0.1009, 0.0895, 0.0928, 0.0943, 0.1001, 0.0990, 0.1115, 0.1057,\n",
            "         0.1143],\n",
            "        [0.0963, 0.1004, 0.0909, 0.0934, 0.0953, 0.1000, 0.1025, 0.1067, 0.1092,\n",
            "         0.1053]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5MVlNMiTanq"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0Y9Q8cwTanq"
      },
      "source": [
        "Model Layers\n",
        "============\n",
        "\n",
        "Let\\'s break down the layers in the FashionMNIST model. To illustrate\n",
        "it, we will take a sample minibatch of 3 images of size 28x28 and see\n",
        "what happens to it as we pass it through the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NvTqq1EbTanq",
        "outputId": "3b733031-2e84-4f53-b176-f3a3e7fb2ce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "jhfq8AR8WIH4",
        "outputId": "27285066-acd2-4669-98e0-8a2463a834c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI_T-JVhTanq"
      },
      "source": [
        "nn.Flatten\n",
        "==========\n",
        "\n",
        "We initialize the\n",
        "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
        "layer to convert each 2D 28x28 image into a contiguous array of 784\n",
        "pixel values ( the minibatch dimension (at dim=0) is maintained).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MxRDCYmcTanq",
        "outputId": "fbf90a35-045b-4141-ff4b-a3b0bcb1b3b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTgfJlVqTanr"
      },
      "source": [
        "nn.Linear\n",
        "=========\n",
        "\n",
        "The [linear\n",
        "layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "is a module that applies a linear transformation on the input using its\n",
        "stored weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gh757DWSTanr",
        "outputId": "47881bb7-ca0c-4476-fc4c-58dca65f4cc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5GNMFidTanr"
      },
      "source": [
        "nn.ReLU\n",
        "=======\n",
        "\n",
        "Non-linear activations are what create the complex mappings between the\n",
        "model\\'s inputs and outputs. They are applied after linear\n",
        "transformations to introduce *nonlinearity*, helping neural networks\n",
        "learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use\n",
        "[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
        "between our linear layers, but there\\'s other activations to introduce\n",
        "non-linearity in your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uqGGnRKNTanr",
        "outputId": "090c13a8-dbce-41e7-b984-d8314e722287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[ 0.1158, -0.0393,  0.7311,  0.0304, -0.4957,  0.2362, -0.5703, -0.2676,\n",
            "          0.1793, -0.5238,  0.2306, -0.0517, -0.4693,  0.1206,  0.2932, -0.2812,\n",
            "         -0.4031,  0.2293, -0.6183,  0.6636],\n",
            "        [ 0.1567,  0.5341,  0.6296,  0.0843, -0.6544, -0.2004, -0.7406, -0.4111,\n",
            "          0.0172, -0.4804, -0.0177, -0.3103, -0.1503,  0.1687,  0.1298, -0.1841,\n",
            "         -0.1287,  0.2168, -0.3309,  0.6146],\n",
            "        [ 0.3450,  0.5454,  0.4025,  0.3941, -0.4671, -0.0454, -0.6965, -0.4040,\n",
            "          0.0700, -0.2801,  0.2408, -0.1693, -0.0897,  0.3951,  0.1377, -0.2144,\n",
            "         -0.3534,  0.4399, -0.3596,  0.4657]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.1158, 0.0000, 0.7311, 0.0304, 0.0000, 0.2362, 0.0000, 0.0000, 0.1793,\n",
            "         0.0000, 0.2306, 0.0000, 0.0000, 0.1206, 0.2932, 0.0000, 0.0000, 0.2293,\n",
            "         0.0000, 0.6636],\n",
            "        [0.1567, 0.5341, 0.6296, 0.0843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0172,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.1687, 0.1298, 0.0000, 0.0000, 0.2168,\n",
            "         0.0000, 0.6146],\n",
            "        [0.3450, 0.5454, 0.4025, 0.3941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0700,\n",
            "         0.0000, 0.2408, 0.0000, 0.0000, 0.3951, 0.1377, 0.0000, 0.0000, 0.4399,\n",
            "         0.0000, 0.4657]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI6bw5GSTanr"
      },
      "source": [
        "nn.Sequential\n",
        "=============\n",
        "\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
        "is an ordered container of modules. The data is passed through all the\n",
        "modules in the same order as defined. You can use sequential containers\n",
        "to put together a quick network like `seq_modules`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "97XiO6FCTanr"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits)"
      ],
      "metadata": {
        "id": "nA05j9ReW-f4",
        "outputId": "93c650dc-34ec-4be9-dc1a-9a6e02eeaf2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1392,  0.4623, -0.0675, -0.3462, -0.0470, -0.1672, -0.2351,  0.1666,\n",
            "          0.1567, -0.2157],\n",
            "        [ 0.1859,  0.3913, -0.2022, -0.3378,  0.0291, -0.1108, -0.1537,  0.2653,\n",
            "          0.1818, -0.2770],\n",
            "        [ 0.1500,  0.3798, -0.0366, -0.2730, -0.0960, -0.1222, -0.1013,  0.2140,\n",
            "          0.1218, -0.1333]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P5N7XZ2Tans"
      },
      "source": [
        "nn.Softmax\n",
        "==========\n",
        "\n",
        "The last linear layer of the neural network returns [logits]{.title-ref}\n",
        "- raw values in \\[-infty, infty\\] - which are passed to the\n",
        "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
        "module. The logits are scaled to values \\[0, 1\\] representing the\n",
        "model\\'s predicted probabilities for each class. `dim` parameter\n",
        "indicates the dimension along which the values must sum to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cwo5x-_jTans"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT2fRmxoTans"
      },
      "source": [
        "Model Parameters\n",
        "================\n",
        "\n",
        "Many layers inside a neural network are *parameterized*, i.e. have\n",
        "associated weights and biases that are optimized during training.\n",
        "Subclassing `nn.Module` automatically tracks all fields defined inside\n",
        "your model object, and makes all parameters accessible using your\n",
        "model\\'s `parameters()` or `named_parameters()` methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and\n",
        "a preview of its values.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.named_parameters()"
      ],
      "metadata": {
        "id": "vL2S50lzXiLq",
        "outputId": "d1814615-ade0-4135-e7fd-3a62a4db295f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.named_parameters at 0x7b06f1218660>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BPv1oOSyTans",
        "outputId": "9c498fd1-a2a1-4ce3-84e1-c56091fcff01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : Parameter containing:\n",
            "tensor([[ 0.0240, -0.0147,  0.0124,  ...,  0.0134, -0.0323,  0.0161],\n",
            "        [-0.0225,  0.0192,  0.0275,  ..., -0.0227, -0.0308, -0.0272],\n",
            "        [-0.0356, -0.0136,  0.0204,  ..., -0.0044,  0.0222, -0.0111],\n",
            "        ...,\n",
            "        [-0.0012, -0.0201, -0.0273,  ..., -0.0162,  0.0258,  0.0061],\n",
            "        [-0.0173, -0.0043, -0.0229,  ..., -0.0315, -0.0266, -0.0029],\n",
            "        [ 0.0241, -0.0294, -0.0212,  ...,  0.0336,  0.0313, -0.0044]],\n",
            "       requires_grad=True) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
            "tensor([-2.2809e-02, -2.4036e-02, -2.5891e-02, -4.5850e-03,  2.6468e-03,\n",
            "         1.4923e-02,  2.4389e-02, -3.2686e-03,  3.3039e-02,  2.6951e-02,\n",
            "        -3.3778e-02, -1.9020e-02, -1.6208e-02,  1.5731e-02, -1.2324e-02,\n",
            "        -2.5640e-02, -1.6783e-02,  6.7507e-03, -1.9217e-02,  3.5428e-02,\n",
            "         1.4183e-02,  5.6835e-03, -3.3873e-02, -1.5056e-02,  5.8225e-03,\n",
            "        -3.2352e-03, -2.2682e-02, -1.1427e-03,  1.0619e-02,  2.3789e-02,\n",
            "         3.1730e-02,  9.8294e-03,  3.5418e-03, -5.1026e-03, -3.4256e-02,\n",
            "         5.0086e-03,  7.0659e-03,  6.3483e-03,  1.8413e-03,  6.0468e-04,\n",
            "        -2.2036e-02,  2.2925e-02,  3.3949e-02,  6.2768e-03, -3.5567e-02,\n",
            "         1.4063e-02, -2.5856e-02, -1.5933e-02, -3.0000e-02,  9.6597e-03,\n",
            "         3.5383e-02, -2.0376e-02, -2.5931e-02,  1.0266e-03, -1.9559e-02,\n",
            "        -3.4048e-02,  1.7380e-02, -1.0541e-02, -2.0208e-02, -1.5796e-02,\n",
            "         2.9572e-02,  3.3118e-02,  2.0456e-02, -1.6152e-02,  1.1068e-02,\n",
            "         1.8150e-02,  2.3984e-02,  2.8966e-02,  1.1513e-02, -3.0687e-02,\n",
            "         3.1279e-02,  2.3566e-02,  8.0572e-03,  9.8211e-03,  2.2302e-02,\n",
            "        -7.4110e-03, -3.6980e-03,  3.1645e-02, -2.1473e-02, -1.0545e-02,\n",
            "         2.4782e-02,  3.4865e-02,  2.8135e-02,  6.3930e-03,  1.2101e-02,\n",
            "         2.4024e-02, -2.4066e-02,  2.8244e-02,  1.0399e-02, -6.7033e-03,\n",
            "        -2.3899e-02, -3.4679e-02, -2.0411e-02, -4.7121e-03,  2.9459e-02,\n",
            "        -2.7368e-02, -2.6548e-02, -1.0054e-02,  8.7246e-03,  5.4161e-04,\n",
            "        -1.2771e-02,  8.1622e-03,  3.1935e-02, -2.2592e-02,  1.5064e-02,\n",
            "        -9.5857e-03, -1.8328e-02, -2.8275e-02,  6.6541e-03,  3.3596e-02,\n",
            "         1.5916e-02, -3.1934e-02, -2.9131e-02,  1.3735e-02,  3.3536e-02,\n",
            "        -1.0383e-02,  2.1949e-02, -3.9058e-03,  1.7908e-02,  2.3237e-02,\n",
            "        -2.3509e-03,  2.8328e-02, -1.8056e-02, -9.0920e-03,  7.9444e-03,\n",
            "         1.6995e-02, -6.0106e-03, -1.3185e-02, -3.9845e-04,  2.2845e-03,\n",
            "         6.4779e-03, -2.5415e-02,  8.8218e-03, -5.5803e-03, -2.3909e-03,\n",
            "        -2.0853e-02, -3.8422e-03,  4.6738e-03, -1.6357e-02, -2.6001e-02,\n",
            "         1.6999e-03, -3.0906e-02, -3.3293e-02,  2.3790e-02,  2.1359e-02,\n",
            "        -3.2802e-02,  1.0110e-02,  1.6901e-02, -3.3263e-02, -2.0405e-02,\n",
            "         1.0859e-02,  2.3658e-02,  1.1746e-02,  7.3202e-03, -3.1439e-02,\n",
            "         8.4285e-03,  3.8348e-04,  1.5672e-02, -5.2607e-03, -2.0223e-02,\n",
            "        -1.2026e-03, -5.9768e-03, -2.4576e-02,  2.7367e-02, -2.5409e-02,\n",
            "        -3.1043e-02, -3.0749e-03,  1.6582e-02,  1.4157e-03,  2.2079e-02,\n",
            "         1.9025e-02, -2.9925e-02, -2.7142e-02, -3.0179e-02,  3.3043e-02,\n",
            "         2.5506e-02, -9.9480e-05, -2.1973e-03,  5.0680e-03, -1.6285e-02,\n",
            "        -2.1158e-02,  1.7422e-02,  3.1653e-02, -1.9673e-02,  3.9521e-03,\n",
            "        -3.3904e-02, -2.4388e-02, -8.1359e-03, -2.2828e-02,  4.9677e-03,\n",
            "        -3.1253e-02, -3.0614e-02,  1.2529e-02,  2.2572e-02,  1.4074e-02,\n",
            "         7.3316e-03,  9.8348e-03,  2.1528e-02,  2.7082e-03,  2.4390e-02,\n",
            "        -2.6445e-02, -7.4847e-03,  3.4704e-02, -3.4643e-02,  1.2935e-03,\n",
            "        -6.5671e-03, -1.2320e-02,  1.2508e-02,  2.9858e-02, -1.6396e-02,\n",
            "        -9.1199e-04, -1.3620e-02,  2.0498e-02,  3.0299e-02,  3.0939e-02,\n",
            "        -1.0432e-02,  2.3257e-02, -1.8059e-02,  2.7369e-02, -1.6073e-02,\n",
            "         1.5351e-02, -2.5810e-02,  5.4860e-03, -3.7927e-03,  1.0947e-02,\n",
            "        -1.8571e-03,  1.6849e-02, -7.8992e-03, -3.3871e-02,  1.8647e-02,\n",
            "        -9.0867e-04, -3.5099e-03, -3.2497e-02, -2.5319e-02,  3.1605e-02,\n",
            "         5.6750e-03, -1.3968e-02,  1.3074e-02,  5.0980e-03,  1.1519e-02,\n",
            "        -6.2352e-04,  2.7620e-02, -3.1529e-02, -2.0701e-02, -2.3327e-02,\n",
            "        -2.0492e-02,  1.1393e-02,  2.4953e-02,  1.6116e-02,  2.0285e-02,\n",
            "         6.4500e-03, -4.9758e-03,  2.4653e-02, -1.1605e-03, -2.9305e-02,\n",
            "         7.1960e-03,  2.4434e-02,  3.3426e-02,  1.9864e-02, -2.5965e-02,\n",
            "        -1.5840e-02,  2.1468e-02,  1.5547e-02, -3.1403e-02,  3.6899e-03,\n",
            "        -2.8278e-02, -1.8200e-02,  2.2988e-02,  2.1517e-02, -1.2782e-02,\n",
            "        -2.9731e-02, -2.2687e-02, -1.8046e-02,  8.8554e-03,  3.2245e-02,\n",
            "        -2.0035e-02,  1.3204e-02,  7.9082e-03,  2.2146e-02, -7.6632e-03,\n",
            "         3.0198e-02,  9.0753e-03, -2.4645e-02, -1.7928e-02, -2.6166e-02,\n",
            "         5.4891e-03, -1.3473e-02, -2.3540e-02,  1.8154e-02,  2.1255e-02,\n",
            "        -2.9035e-02,  3.3195e-02,  3.3154e-02, -5.4774e-03,  1.6162e-02,\n",
            "         1.1793e-02,  1.1037e-02, -3.1061e-02, -1.8969e-02, -9.8322e-03,\n",
            "        -2.6590e-02, -2.9853e-02, -3.5647e-02,  1.8211e-02,  2.9535e-02,\n",
            "        -2.0010e-03, -1.4113e-02,  2.3541e-02,  6.6844e-03, -3.1515e-02,\n",
            "        -2.5930e-02, -2.7248e-02, -1.7228e-02, -4.4923e-03, -1.6797e-02,\n",
            "         6.0284e-03, -1.5767e-02, -4.4463e-03, -1.1651e-02, -1.3750e-02,\n",
            "        -2.7035e-02, -2.5175e-02, -2.7110e-02,  2.5642e-02,  4.5618e-03,\n",
            "         1.2197e-02,  3.1399e-02,  3.3309e-03, -2.3909e-03, -1.3915e-02,\n",
            "        -4.3037e-03, -1.8525e-02, -2.8219e-02, -1.2824e-02, -9.8429e-04,\n",
            "         1.4095e-02, -9.5454e-03,  4.8996e-03, -5.2092e-03, -1.3210e-02,\n",
            "         6.9334e-03,  1.5839e-02, -3.1487e-02,  2.9544e-02,  2.5803e-02,\n",
            "        -3.3643e-03, -2.7625e-02, -4.8943e-03,  1.5160e-02, -3.2142e-02,\n",
            "         3.3308e-02,  2.0708e-02, -6.1173e-03, -1.5527e-02,  4.3630e-03,\n",
            "         1.2856e-02, -8.3839e-03, -2.8563e-02,  1.7039e-02, -1.1418e-02,\n",
            "        -1.8509e-02,  3.4136e-02, -2.3870e-02, -3.0711e-02, -4.9680e-03,\n",
            "        -8.5757e-03, -4.7254e-03,  2.7862e-02,  1.1359e-02,  2.1354e-02,\n",
            "        -2.1747e-02,  3.5686e-02,  3.6941e-03, -1.7763e-02,  2.4620e-02,\n",
            "         1.0247e-03,  3.1575e-02, -3.1174e-02,  3.5407e-02, -1.9734e-03,\n",
            "        -1.3058e-02, -3.4105e-02, -5.6306e-03,  1.8336e-03, -3.2898e-02,\n",
            "         2.5875e-02, -3.0611e-02, -3.0369e-02,  2.3098e-02, -1.5459e-02,\n",
            "         1.9873e-02,  1.4330e-02,  1.4958e-02, -2.0009e-02, -3.2274e-02,\n",
            "         2.5424e-02,  3.4011e-02, -1.6579e-02,  1.2865e-02, -1.8749e-03,\n",
            "         2.7957e-03, -2.9227e-02,  1.3752e-02, -1.8574e-03, -2.9377e-03,\n",
            "        -2.0538e-02, -1.7315e-02, -2.4515e-02, -2.7976e-02,  1.0103e-02,\n",
            "         3.4154e-02, -3.5657e-02, -2.3580e-02, -2.0658e-02, -1.9416e-02,\n",
            "         1.2939e-02,  2.7452e-02, -2.1112e-02, -2.3524e-02, -2.3358e-02,\n",
            "         2.1638e-02, -3.2466e-02,  1.7508e-02, -2.3039e-02, -1.5754e-02,\n",
            "        -2.1109e-02, -3.1022e-02, -3.4033e-02, -2.7770e-03, -3.0831e-02,\n",
            "         2.4476e-02, -1.3133e-04,  2.4331e-02, -2.3454e-02, -1.2903e-02,\n",
            "        -6.3115e-03,  3.3955e-02,  1.7521e-02,  1.9504e-02, -2.1952e-02,\n",
            "        -2.9298e-02,  1.5063e-02,  4.5538e-04,  3.0342e-02,  3.3089e-02,\n",
            "        -2.3318e-02, -2.3504e-02, -3.2972e-03,  1.8264e-02,  5.9493e-03,\n",
            "         2.6701e-02,  2.9427e-02, -2.0737e-03,  2.8802e-02, -2.1882e-02,\n",
            "        -2.6808e-02, -1.1755e-02,  1.4037e-02,  2.3184e-02, -8.5420e-03,\n",
            "         3.1163e-02,  1.9929e-02,  2.5409e-02,  2.9604e-02,  1.6404e-02,\n",
            "        -1.1447e-02, -1.3219e-02,  1.5624e-02,  1.1127e-02,  8.7059e-03,\n",
            "         4.5228e-03, -3.0561e-02,  1.4301e-02,  3.1317e-02,  9.0604e-03,\n",
            "        -2.1616e-02,  1.2296e-02,  2.3694e-02,  2.9675e-02,  4.7294e-03,\n",
            "        -6.6289e-03, -1.6994e-03,  1.1550e-02, -2.3102e-02, -3.5165e-02,\n",
            "        -1.2233e-03, -1.3209e-02, -1.0307e-02,  3.1568e-02,  3.0503e-03,\n",
            "        -8.2627e-03, -3.4677e-02, -3.0684e-02, -1.2633e-02, -2.7975e-03,\n",
            "         6.2772e-03, -3.3333e-02,  8.9594e-03, -1.6143e-03,  5.7103e-03,\n",
            "        -3.1228e-02, -1.4220e-02,  2.7985e-02,  6.0005e-04, -1.0768e-02,\n",
            "        -7.0289e-03,  1.2224e-02,  3.2851e-02, -1.9476e-02,  1.5048e-02,\n",
            "        -2.8466e-03, -1.8747e-02], requires_grad=True) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : Parameter containing:\n",
            "tensor([[ 0.0047, -0.0352,  0.0345,  ...,  0.0377,  0.0308,  0.0308],\n",
            "        [ 0.0385, -0.0426, -0.0115,  ...,  0.0271,  0.0416, -0.0383],\n",
            "        [ 0.0260, -0.0257,  0.0086,  ...,  0.0099,  0.0341, -0.0296],\n",
            "        ...,\n",
            "        [ 0.0044,  0.0366, -0.0250,  ..., -0.0300, -0.0281, -0.0318],\n",
            "        [ 0.0261,  0.0439, -0.0414,  ..., -0.0192, -0.0304,  0.0200],\n",
            "        [ 0.0113, -0.0219,  0.0358,  ..., -0.0032, -0.0380, -0.0321]],\n",
            "       requires_grad=True) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : Parameter containing:\n",
            "tensor([-5.5308e-03, -1.3910e-02,  1.0871e-02,  2.8608e-02, -2.1812e-02,\n",
            "        -2.0855e-03,  1.1260e-02, -2.0729e-02,  4.3012e-02, -1.1865e-02,\n",
            "         3.3415e-02, -3.8889e-03, -1.4230e-02, -1.1106e-02, -7.6184e-03,\n",
            "        -4.3997e-02,  8.8751e-03,  1.8057e-02, -2.1898e-02, -1.7320e-02,\n",
            "        -1.4093e-02,  3.4743e-02, -1.9431e-02, -3.3582e-02,  2.8899e-02,\n",
            "         6.0116e-03,  4.0047e-02,  3.4957e-02,  1.3574e-02, -9.4840e-03,\n",
            "         2.2321e-02, -2.9447e-02,  4.0535e-02,  2.7687e-02, -3.5214e-03,\n",
            "        -1.1631e-02, -2.5641e-02,  3.9965e-02, -2.2850e-02,  1.6097e-02,\n",
            "        -4.1334e-02, -2.9314e-02, -8.3449e-03,  1.8262e-02,  2.1492e-02,\n",
            "        -1.7380e-02,  2.2374e-02,  3.0004e-02,  2.8408e-02,  2.1602e-02,\n",
            "        -7.7881e-04, -3.8071e-04, -1.5710e-02, -1.5124e-02, -6.8043e-03,\n",
            "        -7.7504e-03, -2.4798e-02,  3.9403e-02,  3.3491e-02, -2.5210e-02,\n",
            "         4.4102e-02,  2.7804e-02, -1.6009e-02, -1.0274e-02,  8.1602e-03,\n",
            "        -3.7061e-02,  4.3662e-02,  1.0624e-02,  3.2258e-02,  1.1220e-02,\n",
            "        -2.0778e-02,  4.3978e-02, -1.5812e-02, -7.4599e-03,  1.8604e-02,\n",
            "         4.3193e-02,  4.1026e-02, -3.3604e-02,  4.3268e-02, -3.4488e-02,\n",
            "        -7.8290e-03,  1.6845e-02,  2.3459e-02, -8.6619e-03,  3.9386e-02,\n",
            "         1.2363e-02,  4.6169e-03,  3.8032e-02, -2.8178e-02,  3.0173e-02,\n",
            "        -1.8200e-03, -5.2521e-03,  1.5158e-02, -2.0492e-02,  3.5798e-02,\n",
            "        -3.7636e-03,  2.8601e-02, -1.5345e-02, -3.3531e-02, -3.3918e-02,\n",
            "         2.4372e-02,  1.3124e-02,  4.3435e-02, -1.8863e-02,  8.4547e-03,\n",
            "         3.8709e-02, -2.9755e-02,  8.2043e-03,  2.7738e-02,  4.2523e-02,\n",
            "        -1.8513e-02, -9.0977e-04,  4.2159e-02,  3.2036e-02, -1.1730e-02,\n",
            "        -3.2365e-02,  3.8730e-02,  2.5541e-02,  1.6202e-02, -2.9094e-02,\n",
            "         1.8947e-02,  1.7560e-02, -1.6791e-02, -9.9016e-03,  1.6537e-02,\n",
            "         1.3619e-02,  2.4178e-02, -2.3655e-02, -2.7381e-02,  2.2727e-02,\n",
            "        -9.4523e-04, -3.5881e-02,  1.8688e-02,  3.2004e-02,  4.1724e-02,\n",
            "        -2.0625e-02, -3.9647e-02, -2.8195e-02, -1.8224e-04, -1.0433e-02,\n",
            "         8.2508e-04, -6.8038e-03,  1.5839e-02,  3.9239e-02, -4.4008e-02,\n",
            "         5.5033e-03, -1.2716e-02,  4.8418e-03, -9.0248e-03, -4.2814e-03,\n",
            "        -1.1266e-02, -2.5385e-02,  9.3189e-03,  3.5032e-02,  2.3919e-02,\n",
            "         2.5359e-02,  2.3702e-02,  1.0715e-02,  1.6949e-02,  3.0862e-02,\n",
            "         1.5731e-02, -2.7381e-02, -2.0864e-02, -4.3588e-02, -1.8743e-02,\n",
            "        -4.2576e-02, -2.9226e-02, -6.3142e-03, -3.5760e-02,  4.9786e-03,\n",
            "        -3.6380e-02, -2.2574e-02, -1.2436e-02,  2.3340e-03,  2.3521e-03,\n",
            "         2.2704e-02,  5.9106e-03, -3.3350e-02,  1.4731e-02,  3.0993e-02,\n",
            "        -2.9897e-02,  5.5337e-03,  8.4740e-03, -3.3094e-02,  3.4333e-02,\n",
            "         2.8815e-02, -4.3377e-02,  3.8063e-02,  3.4721e-02, -3.1732e-02,\n",
            "        -2.5250e-02,  4.1839e-02, -2.7294e-02, -3.7524e-02,  1.6360e-02,\n",
            "         1.8507e-02, -3.1281e-02, -3.1699e-02, -1.4919e-02, -3.3737e-03,\n",
            "         8.6734e-03, -9.5093e-03,  1.8422e-02, -8.7373e-03,  2.1412e-02,\n",
            "        -1.6468e-02, -4.2627e-02, -2.7484e-02, -4.3424e-02,  3.4162e-02,\n",
            "         8.5928e-03, -2.7940e-02, -4.2701e-02, -2.8212e-02, -2.6557e-02,\n",
            "        -2.6044e-02,  5.9502e-03, -1.4892e-02,  3.7577e-02,  3.8050e-02,\n",
            "        -1.0756e-03, -2.1743e-02,  1.4815e-02, -1.4027e-02,  3.3204e-02,\n",
            "        -7.6515e-03, -1.1530e-02, -1.5782e-02, -2.5485e-02,  4.0806e-02,\n",
            "         3.9283e-02,  1.7929e-02,  2.8177e-02,  4.1247e-02, -3.6978e-02,\n",
            "        -2.9165e-02, -1.8902e-02,  3.6250e-02, -5.8029e-03, -4.3379e-02,\n",
            "         8.9432e-03,  1.6843e-02, -1.6467e-02,  4.9360e-04,  3.7448e-02,\n",
            "         8.7215e-03,  2.9637e-02,  2.4360e-02, -3.8381e-02, -2.5068e-02,\n",
            "         1.8974e-02,  1.0181e-02,  2.7223e-02, -2.4325e-02,  2.6918e-02,\n",
            "        -1.9440e-02,  8.3785e-03,  8.1780e-03,  4.2850e-02,  1.0736e-02,\n",
            "         1.0080e-02, -1.3916e-02, -3.1479e-02, -1.3055e-02,  2.0008e-02,\n",
            "        -1.6894e-02, -1.5744e-02, -3.2916e-02, -1.6635e-02, -3.1038e-02,\n",
            "        -4.3570e-02,  6.0592e-03, -1.3009e-02,  8.1440e-03, -2.9887e-02,\n",
            "         3.1226e-02,  2.7573e-02, -3.1878e-02, -3.8631e-02,  7.0914e-03,\n",
            "        -2.2419e-02, -3.4329e-02, -2.4372e-02, -1.8631e-02, -3.6177e-02,\n",
            "        -6.5852e-03,  3.9097e-02,  4.6025e-03,  2.2256e-02,  4.3462e-02,\n",
            "         3.8068e-02,  2.8735e-02,  1.1536e-02, -2.7458e-02, -2.3616e-03,\n",
            "         9.9260e-03, -3.7994e-02, -3.8602e-02,  3.2271e-02, -4.3937e-02,\n",
            "        -1.6338e-02, -3.2444e-02,  3.3506e-03,  8.4983e-04,  1.7321e-02,\n",
            "         1.1375e-02,  3.6255e-02, -6.3579e-03,  1.7103e-02,  2.4806e-02,\n",
            "        -1.0294e-02, -2.1663e-02,  3.3615e-02, -4.0601e-02,  9.4699e-03,\n",
            "         6.5559e-03, -1.3122e-02,  1.1110e-02,  1.6421e-02, -1.1483e-02,\n",
            "        -5.2530e-04, -4.7691e-03,  1.5618e-02,  1.8262e-02,  1.1014e-02,\n",
            "        -7.5788e-03,  1.7857e-02, -7.1874e-03,  3.0100e-02,  1.3020e-02,\n",
            "        -5.8045e-03,  1.8993e-02, -3.5163e-02,  2.7633e-02,  2.3608e-02,\n",
            "         3.3649e-02,  2.0558e-02, -3.7139e-02, -4.2447e-03,  2.8160e-02,\n",
            "        -1.0265e-04, -1.6992e-02, -2.8255e-02, -3.3252e-02, -2.5655e-04,\n",
            "        -9.8520e-03, -2.8749e-02, -1.3670e-02, -1.8726e-02, -7.8983e-03,\n",
            "         4.3462e-02, -3.6191e-02,  4.0165e-02,  4.0606e-02, -2.4820e-02,\n",
            "         3.4315e-02, -1.0640e-02, -2.8384e-02, -4.2838e-03,  3.2006e-03,\n",
            "         1.0037e-02,  4.0635e-02, -3.6388e-02, -1.8310e-02, -4.6752e-03,\n",
            "        -1.7419e-02, -2.0424e-02, -3.8572e-02, -3.0025e-02, -2.3451e-02,\n",
            "         2.2114e-02,  3.9701e-02, -1.1386e-02, -2.5156e-02, -2.5630e-02,\n",
            "        -2.4395e-02, -3.0308e-02,  4.1393e-02, -2.9219e-03, -3.5377e-02,\n",
            "        -1.6881e-02, -1.4186e-03,  1.4008e-02,  3.3757e-02,  3.9062e-02,\n",
            "         4.4091e-02,  3.9907e-02,  8.9060e-03,  3.2312e-02, -7.8800e-03,\n",
            "        -3.8996e-02, -2.8111e-02, -1.5815e-03, -3.4723e-02, -2.5922e-02,\n",
            "         1.2279e-02,  3.1987e-02, -1.7879e-02, -8.8531e-03, -3.6604e-02,\n",
            "         4.3932e-02, -2.9481e-02, -2.7386e-03, -2.8667e-02,  3.2260e-02,\n",
            "         8.0738e-03, -2.3119e-02, -2.5610e-02,  2.0284e-02,  8.7796e-03,\n",
            "        -3.8270e-02,  3.9169e-02,  3.5124e-02,  1.0355e-03,  3.3059e-02,\n",
            "         3.0878e-03, -4.1075e-02,  4.0665e-02, -1.0441e-02,  1.7360e-02,\n",
            "         1.0413e-02, -2.2121e-02, -3.3858e-03,  3.6124e-02, -4.2974e-03,\n",
            "         2.2481e-02, -1.5191e-03,  3.3337e-02,  2.7383e-02, -2.9259e-02,\n",
            "         2.5329e-02, -3.6521e-02,  3.6752e-02, -5.7606e-03, -2.6029e-02,\n",
            "         1.0612e-03,  1.8054e-02,  1.9298e-02, -9.2366e-03, -5.2152e-03,\n",
            "        -4.2779e-02,  3.3850e-02, -4.3848e-02, -3.9913e-02, -2.7376e-02,\n",
            "         3.2936e-03, -4.0203e-02, -3.0613e-02,  2.7037e-03, -1.8056e-02,\n",
            "        -3.3271e-02, -3.6488e-02,  3.4327e-02,  2.6190e-02,  1.2468e-02,\n",
            "         5.2305e-03,  3.9744e-02, -3.2758e-02,  1.3614e-02,  2.2223e-02,\n",
            "         2.0173e-02, -1.2485e-02,  4.1538e-02, -1.6314e-02, -1.5693e-02,\n",
            "         1.8490e-02,  1.5878e-02,  4.2876e-02, -1.4816e-03, -5.3325e-03,\n",
            "         1.2206e-02, -3.8580e-02,  1.4877e-02,  1.4848e-02,  3.3198e-02,\n",
            "        -2.1031e-02, -1.9680e-02, -4.2110e-02,  3.3245e-02,  2.7681e-02,\n",
            "         1.7360e-02,  4.2905e-02,  7.7845e-05, -1.4156e-02, -1.1103e-02,\n",
            "        -8.1955e-03,  1.5405e-04, -4.3225e-02,  3.8713e-02,  2.1216e-03,\n",
            "         3.7466e-02,  1.9755e-02,  6.6149e-03,  3.4050e-02,  4.1330e-02,\n",
            "         2.8663e-02, -2.0739e-02,  1.2815e-02,  1.8941e-02,  2.1588e-02,\n",
            "         1.3743e-02,  6.0908e-03,  3.2596e-02,  2.2663e-02, -3.5383e-02,\n",
            "         7.1008e-03, -3.6559e-02,  2.4077e-02,  3.5667e-02,  1.4114e-02,\n",
            "        -6.8461e-03,  3.8122e-02], requires_grad=True) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : Parameter containing:\n",
            "tensor([[ 0.0314,  0.0025, -0.0322,  ..., -0.0045,  0.0181,  0.0183],\n",
            "        [-0.0093, -0.0297, -0.0056,  ..., -0.0200, -0.0146, -0.0026],\n",
            "        [-0.0206, -0.0023, -0.0249,  ..., -0.0415, -0.0126,  0.0180],\n",
            "        ...,\n",
            "        [-0.0135, -0.0132,  0.0371,  ...,  0.0292, -0.0060,  0.0356],\n",
            "        [-0.0367, -0.0046, -0.0195,  ..., -0.0246,  0.0166,  0.0420],\n",
            "        [-0.0325,  0.0106, -0.0413,  ..., -0.0374,  0.0375, -0.0384]],\n",
            "       requires_grad=True) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : Parameter containing:\n",
            "tensor([ 0.0380,  0.0122, -0.0430, -0.0299,  0.0044,  0.0111,  0.0364, -0.0113,\n",
            "         0.0250,  0.0423], requires_grad=True) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXR2RjqkTans"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3vR_Tr1Tans"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "-   [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}